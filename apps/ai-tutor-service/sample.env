LLM_API_KEY=""
LLM_NAME="gemini/gemini-flash-lite-latest"
LLM_API_BASE=""

PHOENIX_COLLECTOR_ENDPOINT="http://localhost:6006"

TUTOR_API_KEY=""

# Model used by the LLM judge in eval runs.
# Recommended options (see README for guidance):
#   gemma3:4b    — fast, works on M1 16GB and most hardware (~3GB VRAM Q4)
#   qwen2.5:7b   — better JSON adherence, recommended for 16GB VRAM GPU
#   gemma3:12b   — higher quality, requires ~8GB VRAM
OLLAMA_JUDGE_MODEL="gemma3:4b"
OLLAMA_BASE_URL="http://localhost:11434"
